{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SoG0dO88y4W",
    "outputId": "a780bee8-1443-4b19-97c6-003fea8ebf3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive  #in case test data is on the drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell #1 Input test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E32nJ6dLp5Hf",
    "outputId": "8a7a38f3-beef-4d98-8aa7-ea6820819641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a k c f a d a g b e a h c d a h a k a k b f b e b e b d c d a e c d b f \n",
      "c f b e c d b f b e a k g h b e a k i j b d c d a h k l m c d a e ee a h f ed ef a g e eg b f a d eh ei a k d ej \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------- (*)required to run this cell---------------------------------------------------------------------\n",
    "\n",
    "#---------------------------note: the file includes all the code. training part is commented. Since the code was developed for training naming might be confusing.\n",
    "#---------------------------Instructions what needs to be changed and run is give in comments\n",
    "import pickle                                                     #read input and output languages  \n",
    "\n",
    "train_input = pickle.load(open('/content/drive/MyDrive/project2.0/DS_5_train_input','rb'))    #--------------------------> change dataset directory \n",
    "train_output = pickle.load(open('/content/drive/MyDrive/project2.0/DS_5_train_output','rb'))  #--------------------------> change dataset directory \n",
    "\n",
    "print(train_input[2])\n",
    "print(train_output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hc36aECBb-xQ",
    "outputId": "87dd3d31-191a-48e8-bdd2-ba07c30a3dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum output language text length: 95\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------- (*)required to run this cell---------------------------------------------------------------------\n",
    "import tensorflow as tf                                                \n",
    "import string\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "vocab_size = 500                                                     # For time limit assume 500 words\n",
    "sequence_length = max([len(train_output[i].split()) for i in range(len(train_output))])  #maximum length of a sentence in the dataset\n",
    "\n",
    "print(\"Maximum output language text length: \"+str(sequence_length))\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1\n",
    ")\n",
    "\n",
    "source_vectorization.adapt(train_input)           \n",
    "target_vectorization.adapt(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g87agpypg5qh"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------- (*)required to run this cell---------------------------------------------------------------------\n",
    "text_pairs = list()                                                          #creating list of text pairs\n",
    "for n in range(len(train_input)):\n",
    "    text_pairs.append((train_input[n], \"[start] \"+train_output[n]+\" [end]\")) # adding [start] and [end] to the translatin \n",
    "\n",
    "'''\n",
    "import random\n",
    "random.shuffle(text_pairs)                                                    #suffle and divide into train, val and test\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "'''\n",
    "test_pairs = text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "8kbGSdBjfV9M",
    "outputId": "bd3d8484-8f43-491a-8971-5017c5960e6e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nbatch_size = 64\\n\\ndef format_dataset(lang1, lang2):\\n    lang1 = source_vectorization(lang1)\\n    lang2 = target_vectorization(lang2)\\n    return ({\\n        \"language1\": lang1,\\n        \"language2\": lang2[:, :-1],\\n    }, lang2[:, 1:])\\n\\ndef make_dataset(pairs):\\n    lang1_texts, lang2_texts = zip(*pairs)\\n    lang1_texts = list(lang1_texts)\\n    lang2_texts = list(lang2_texts)\\n    dataset = tf.data.Dataset.from_tensor_slices((lang1_texts, lang2_texts))\\n    dataset = dataset.batch(batch_size)\\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\\n    return dataset.shuffle(2048).prefetch(16).cache()\\n\\ntrain_ds = make_dataset(train_pairs)       #creating training and pal pairs for training the model in batches \\nval_ds = make_dataset(val_pairs)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(lang1, lang2):\n",
    "    lang1 = source_vectorization(lang1)\n",
    "    lang2 = target_vectorization(lang2)\n",
    "    return ({\n",
    "        \"language1\": lang1,\n",
    "        \"language2\": lang2[:, :-1],\n",
    "    }, lang2[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    lang1_texts, lang2_texts = zip(*pairs)\n",
    "    lang1_texts = list(lang1_texts)\n",
    "    lang2_texts = list(lang2_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((lang1_texts, lang2_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)       #creating training and pal pairs for training the model in batches \n",
    "val_ds = make_dataset(val_pairs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "Eb7Ig4tjiaa5",
    "outputId": "2a03e0e2-7efe-4db7-d808-7476332c87f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nfor inputs, targets in train_ds.take(1):                            #checking out dataset shape (not required for testing)\\n    print(f\"inputs[\\'language1\\'].shape: {inputs[\\'language1\\'].shape}\")\\n    print(f\"inputs[\\'language2\\'].shape: {inputs[\\'language2\\'].shape}\")\\n    print(f\"targets.shape: {targets.shape}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for inputs, targets in train_ds.take(1):                            #checking out dataset shape (not required for testing)\n",
    "    print(f\"inputs['language1'].shape: {inputs['language1'].shape}\")\n",
    "    print(f\"inputs['language2'].shape: {inputs['language2'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "pCWp3mUAitpF",
    "outputId": "d5ae2942-5a13-4da7-cfb2-26c18971833a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nembed_dim = 256                            #building up a model - takes first language a an input\\nlatent_dim = 1024\\n\\nsource = keras.Input(shape=(None,), dtype=\"int64\", name=\"language1\")\\nx = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\\nencoded_source = layers.Bidirectional(\\n    layers.GRU(latent_dim), merge_mode=\"sum\")(x)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "embed_dim = 256                            #building up a model - takes first language a an input\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"language1\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "pch7LfEbnEHu",
    "outputId": "84860a00-ffd5-417a-d3b0-b3159cdc2519"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\npast_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"language2\") #branch that takes target language as an input \\nx = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)  #writing the sentence in 0s and 1s\\ndecoder_gru = layers.GRU(latent_dim, return_sequences=True)\\nx = decoder_gru(x, initial_state=encoded_source)\\nx = layers.Dropout(0.5)(x)\\ntarget_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\\nseq2seq_rnn = keras.Model([source, past_target], target_next_step)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"language2\") #branch that takes target language as an input \n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)  #writing the sentence in 0s and 1s\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell #2 Input model durectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_8_wj95BnDRz"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------- (*)required to run this cell---------------------------------------------------------------------\n",
    "'''\n",
    "seq2seq_rnn.compile(                                                                                 #choosing model's accuracy and loss\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "'''\n",
    "seq2seq_rnn = keras.models.load_model('/content/drive/MyDrive/project2.0/project2_5.keras')      #------------------------------------->model directory\n",
    "#callbacks = [keras.callbacks.ModelCheckpoint(\"project2_5.keras\",                                     #saving updated model \n",
    "#                                    save_best_only=True)]                                            #training the model \n",
    "#seq2seq_rnn.fit(train_ds, epochs=5, validation_data=val_ds, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SO8HVpdnS-H",
    "outputId": "2ddfe7d4-8a98-4bd3-ee5d-d2fa2039cdd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: ['b', 'f', 'c', 'f', 'b', 'f', 'c', 'd', 'a', 'j', 'e', 'f', 'g', 'c', 'e', 'b', 'g', 'a', 'k', 'i', 'j', 'b', 'd', 'b', 'f', 'a', 'k', 'l', 'm', 'b', 'f', 'b', 'd', 'a', 'h', 'ed', 'ee', 'ef', 'a', 'k', 'k', 'eg', 'a', 'k', 'h', 'eh', 'a', 'e', 'ei', 'c', 'd', 'a', 'f', 'ej', 'ek', 'a', 'g', 'd', 'el']\n",
      "prediction: ['b', 'f', 'c', 'f', 'b', 'f', 'c', 'd', 'a', 'j', 'e', 'f', 'g', 'c', 'e', 'b', 'g', 'b', 'd', 'b', 'g', 'a', 'k', 'k', 'l', 'a', 'k', 'j', 'm', 'b', 'f', 'a', 'k', 'ed', 'ee', 'b', 'd', 'a', 'd', 'ef', 'eg', 'a', 'f', 'i', 'eh', 'a', 'k', 'h', 'ei', 'a', 'e', 'ej', 'a', 'k', 'd', 'ek']\n",
      "accuracy for 1samples: 0.3898305084745763\n",
      "ground truth: ['b', 'e', 'c', 'd', 'b', 'g', 'c', 'g', 'a', 'i', 'e', 'f', 'g', 'a', 'f', 'd', 'h', 'c', 'e', 'c', 'f', 'c', 'f', 'a', 'e', 'l', 'c', 'd', 'a', 'i', 'k', 'm', 'ed', 'a', 'i', 'i', 'j', 'ee', 'a', 'e', 'ef', 'a', 'e', 'eg', 'a', 'e', 'eh', 'c', 'd', 'a', 'k', 'ei', 'ej']\n",
      "prediction: ['b', 'e', 'c', 'd', 'b', 'g', 'c', 'g', 'a', 'i', 'e', 'f', 'g', 'a', 'f', 'd', 'h', 'c', 'e', 'c', 'f', 'a', 'e', 'k', 'a', 'k', 'j', 'l', 'c', 'f', 'a', 'd', 'm', 'ed', 'c', 'f', 'a', 'd', 'ee', 'ef', 'a', 'e', 'eg', 'a', 'e', 'eh', 'c', 'd', 'a', 'd', 'ei', 'ej', 'a', 'k', 'i', 'ek', 'a', 'e', 'el']\n",
      "accuracy for 2samples: 0.39285714285714285\n",
      "ground truth: ['c', 'f', 'b', 'e', 'c', 'd', 'b', 'f', 'b', 'e', 'a', 'k', 'g', 'h', 'b', 'e', 'a', 'k', 'i', 'j', 'b', 'd', 'c', 'd', 'a', 'h', 'k', 'l', 'm', 'c', 'd', 'a', 'e', 'ee', 'a', 'h', 'f', 'ed', 'ef', 'a', 'g', 'e', 'eg', 'b', 'f', 'a', 'd', 'eh', 'ei', 'a', 'k', 'd', 'ej']\n",
      "prediction: ['c', 'f', 'b', 'e', 'c', 'd', 'b', 'f', 'b', 'e', 'a', 'k', 'g', 'h', 'b', 'e', 'b', 'd', 'a', 'k', 'j', 'k', 'c', 'd', 'a', 'd', 'l', 'm', 'c', 'd', 'a', 'k', 'ed', 'ee', 'b', 'd', 'a', 'd', 'ef', 'eg', 'a', 'h', 'f', 'i', 'eh', 'a', 'd', 'e', 'ei', 'a', 'd', 'd', 'ej']\n",
      "accuracy for 3samples: 0.4303030303030303\n",
      "ground truth: ['b', 'g', 'b', 'e', 'c', 'g', 'a', 'g', 'e', 'f', 'c', 'g', 'a', 'j', 'd', 'g', 'h', 'b', 'g', 'b', 'g', 'b', 'f', 'a', 'g', 'k', 'l', 'b', 'e', 'a', 'k', 'm', 'ed', 'a', 'j', 'i', 'j', 'ee', 'c', 'e', 'c', 'e', 'a', 'h', 'ef', 'eg', 'eh']\n",
      "prediction: ['b', 'g', 'b', 'e', 'c', 'g', 'a', 'g', 'e', 'f', 'c', 'g', 'a', 'j', 'd', 'g', 'h', 'b', 'g', 'b', 'g', 'b', 'f', 'b', 'g', 'a', 'g', 'l', 'm', 'a', 'k', 'k', 'ed', 'c', 'f', 'a', 'k', 'ee', 'ef', 'a', 'k', 'j', 'eg', 'b', 'g', 'a', 'h', 'i', 'eh', 'ei']\n",
      "accuracy for 4samples: 0.4669811320754717\n",
      "ground truth: ['b', 'd', 'c', 'g', 'c', 'e', 'a', 'f', 'e', 'f', 'c', 'd', 'a', 'd', 'g', 'h', 'c', 'f', 'c', 'f', 'a', 'j', 'i', 'j', 'k', 'a', 'e', 'l', 'c', 'f', 'a', 'k', 'm', 'ed', 'c', 'd', 'c', 'd', 'a', 'f', 'ef', 'eg', 'b', 'g', 'c', 'g', 'a', 'g', 'ei', 'ej', 'c', 'e', 'a', 'h', 'eh', 'ek', 'el', 'a', 'g', 'ee', 'em', 'b', 'd', 'c', 'e', 'a', 'h', 'fd', 'fe', 'ff', 'a', 'k', 'd', 'fg']\n",
      "prediction: ['b', 'd', 'c', 'g', 'c', 'e', 'a', 'd', 'e', 'f', 'c', 'd', 'a', 'f', 'g', 'h', 'c', 'f', 'c', 'f', 'a', 'd', 'j', 'k', 'c', 'd', 'c', 'e', 'a', 'd', 'm', 'ed', 'c', 'd', 'a', 'e', 'ef', 'a', 'd', 'ee', 'eg', 'a', 'd', 'l', 'eh', 'a', 'f', 'i', 'ei', 'b', 'e', 'c', 'd', 'a', 'd', 'ek', 'el', 'a', 'f', 'ej', 'em', 'a', 'k', 'd', 'fd']\n",
      "accuracy for 5samples: 0.43006993006993005\n",
      "accuracy: 0.43006993006993005\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------- (*)required to run this cell---------------------------------------------------------------------\n",
    "from inspect import CORO_CREATED\n",
    "import numpy as np\n",
    "lang2_vocab = target_vectorization.get_vocabulary()\n",
    "lang2_index_lookup = dict(zip(range(len(lang2_vocab)), lang2_vocab))\n",
    "max_decoded_sentence_length = 95\n",
    "\n",
    "def decode_sequence(input_sentence):                                   #returns translated sentence for each input \n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])  #write words as numbers\n",
    "    decoded_sentence = \"[start]\"                                       #initialize the target sentence \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence]) #write words as numbers\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence]) #predicts next token given the current translation and input\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :]) #numerical index of the predicted word\n",
    "        sampled_token = lang2_index_lookup[sampled_token_index] #predicted word \n",
    "        decoded_sentence += \" \" + sampled_token  #add to translation \n",
    "        if sampled_token == \"[end]\": \n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "#number_of_tests = 5 #in case of early break uncomment if statement\n",
    "for  i,  pair in enumerate(test_pairs):\n",
    "    ground_truth = pair[1].split()                  #true translation \n",
    "    prediction = decode_sequence(pair[0]).split()   #predicted translation \n",
    "    total+=len(ground_truth)-2                      #total number of words in true translation \n",
    "    for j in range(len(ground_truth)-1):\n",
    "        if len(prediction) > j and j!=0:\n",
    "            if ground_truth[j] == prediction[j]:\n",
    "                correct+=1                          #correctly translated words in prediction \n",
    "    print(\"ground truth: \"+str(ground_truth[1:len(ground_truth)-1])) #printing out true translation and prediction \n",
    "    print(\"prediction: \"+str(prediction[1:len(prediction)-1]))\n",
    "    print(\"accuracy for \"+str(i+1)+\" samples: \"+str(correct/(1.0*total))) #updated accuracy after one translation\n",
    "    #if i == number_of_tests-1:\n",
    "        #break\n",
    "print(\"accuracy: \"+str(correct/(1.0*total))) #total accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project2_read.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
